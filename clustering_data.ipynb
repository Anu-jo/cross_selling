{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anu-jo/cross_selling/blob/main/clustering_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qPOUNwqS620",
        "outputId": "6b4f5813-33cf-4d2a-8ed0-1b2f6d674813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d206861ababf>:16: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in log1p\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.impute import KNNImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'Test.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Select numeric and non-numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "# Initial Summary\n",
        "\n",
        "# Check for missing values\n",
        "missing_values_before = df.isna().sum()\n",
        "\n",
        "# Check for skewness\n",
        "skewness_before = df[numeric_cols].skew()\n",
        "\n",
        "# Detect outliers using the IQR method\n",
        "def detect_outliers_iqr(data):\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR)))\n",
        "    return outliers\n",
        "\n",
        "outliers_iqr_before = df[numeric_cols].apply(detect_outliers_iqr, axis=0)\n",
        "outliers_count_iqr_before = outliers_iqr_before.sum()\n",
        "\n",
        "# Detect outliers using Z-score\n",
        "z_scores_before = np.abs(stats.zscore(df[numeric_cols].dropna()))\n",
        "outliers_zscore_before = (z_scores_before > 3).sum(axis=0)\n",
        "\n",
        "# Handling Missing Values\n",
        "\n",
        "# Technique 1: Imputation with Mean/Median/Mode\n",
        "for col in numeric_cols:\n",
        "    df[col].fillna(df[col].mean(), inplace=True)  # Mean imputation for numeric columns\n",
        "\n",
        "for col in non_numeric_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)  # Mode imputation for non-numeric columns\n",
        "\n",
        "# Technique 2: Model-Based Imputation (KNN)\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df[numeric_cols] = knn_imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Ensure no missing values remain\n",
        "df.fillna(method='ffill', inplace=True)  # Forward fill as a last resort\n",
        "df.fillna(method='bfill', inplace=True)  # Backward fill as a last resort\n",
        "\n",
        "# Handling Outliers\n",
        "\n",
        "# Technique 1: Clipping\n",
        "def clip_outliers(data):\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return data.clip(lower_bound, upper_bound)\n",
        "\n",
        "df[numeric_cols] = df[numeric_cols].apply(clip_outliers)\n",
        "\n",
        "# Technique 2: Transformation (Log Transformation)\n",
        "# Apply log transformation to reduce the effect of outliers\n",
        "df[numeric_cols] = df[numeric_cols].apply(lambda x: np.log1p(x))\n",
        "\n",
        "# After Treatment: Summary Checks\n",
        "\n",
        "# Check for missing values\n",
        "missing_values_after = df.isna().sum()\n",
        "\n",
        "# Check for skewness\n",
        "skewness_after = df[numeric_cols].skew()\n",
        "\n",
        "# Detect outliers using the IQR method after treatment\n",
        "outliers_iqr_after = df[numeric_cols].apply(detect_outliers_iqr, axis=0)\n",
        "outliers_count_iqr_after = outliers_iqr_after.sum()\n",
        "\n",
        "# Detect outliers using Z-score after treatment\n",
        "z_scores_after = np.abs(stats.zscore(df[numeric_cols].dropna()))\n",
        "outliers_zscore_after = (z_scores_after > 3).sum(axis=0)\n",
        "\n",
        "\"\"\"# Summary Comparison\n",
        "\n",
        "# Combine the before and after summaries into DataFrames\n",
        "summary_missing_values = pd.DataFrame({\n",
        "    'Before Treatment': missing_values_before,\n",
        "    'After Treatment': missing_values_after\n",
        "})\n",
        "summary_skewness = pd.DataFrame({\n",
        "    'Before Treatment': skewness_before,\n",
        "    'After Treatment': skewness_after\n",
        "})\n",
        "summary_outliers_iqr = pd.DataFrame({\n",
        "    'Before Treatment': outliers_count_iqr_before,\n",
        "    'After Treatment': outliers_count_iqr_after\n",
        "})\n",
        "summary_outliers_zscore = pd.DataFrame({\n",
        "    'Before Treatment': outliers_zscore_before,\n",
        "    'After Treatment': outliers_zscore_after\n",
        "})\n",
        "\n",
        "# Print the summaries\n",
        "print(\"\\nMissing Values Summary:\\n\", summary_missing_values[summary_missing_values.sum(axis=1) > 0])\n",
        "print(\"\\nSkewness Summary:\\n\", summary_skewness[(summary_skewness['Before Treatment'].abs() > 1) | (summary_skewness['After Treatment'].abs() > 1)])\n",
        "print(\"\\nOutliers (IQR method) Summary:\\n\", summary_outliers_iqr[summary_outliers_iqr.sum(axis=1) > 0])\n",
        "print(\"\\nOutliers (Z-score method) Summary:\\n\", summary_outliers_zscore[summary_outliers_zscore.sum(axis=1) > 0])\n",
        "\"\"\"\n",
        "# Clustering\n",
        "\n",
        "# Identify numerical and categorical columns again after preprocessing\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "# Preprocessing pipelines for numerical and categorical data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply the preprocessing pipeline to the data\n",
        "X_preprocessed = preprocessor.fit_transform(df)\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')  # Or 'median' or 'most_frequent' if appropriate\n",
        "X_preprocessed  = imputer.fit_transform(X_preprocessed )\n",
        "\n",
        "# K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_preprocessed)\n",
        "kmeans_silhouette = silhouette_score(X_preprocessed, kmeans_labels)\n",
        "print(f'K-Means Silhouette Score: {kmeans_silhouette}')\n",
        "\n",
        "# Hierarchical Clustering\n",
        "agglo = AgglomerativeClustering(n_clusters=5)\n",
        "agglo_labels = agglo.fit_predict(X_preprocessed)\n",
        "agglo_silhouette = silhouette_score(X_preprocessed, agglo_labels)\n",
        "print(f'Agglomerative Clustering Silhouette Score: {agglo_silhouette}')\n",
        "\n",
        "# DBSCAN Clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X_preprocessed)\n",
        "# Evaluate only non-noise points\n",
        "if len(set(dbscan_labels)) > 1:\n",
        "    dbscan_silhouette = silhouette_score(X_preprocessed[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])\n",
        "    print(f'DBSCAN Silhouette Score: {dbscan_silhouette}')\n",
        "else:\n",
        "    print('DBSCAN resulted in only one cluster.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOreLEeVFVaEEvYo5mwGAs7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}